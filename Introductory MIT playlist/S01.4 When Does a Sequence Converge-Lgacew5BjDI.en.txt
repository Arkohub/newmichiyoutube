 So we looked at the formal definition of what it means  for a sequence to converge, but as a practical matter, how  can we tell whether a given sequence converges or not?  There are two criteria that are the most commonly used for  that purpose, and it's useful to be aware of them.  The first one deals with the case where we have a sequence  of numbers that keep increasing, or at least, they  do not go down.  In that case, those numbers may go up forever without any  bound, so if you look at any particular value, there's  going to be a time at which the sequence has  exceeded that value.  In that case, we say that the sequence  converges to infinity.  But if this is not the case, if it does not converge to  infinity, which means that the entries of the  sequence are bounded--  they do not grow arbitrarily large--  then, in that case, it is guaranteed that the sequence  will converge to a certain number.  This is not something that we will attempt to prove, but it  is a useful fact to know.  Another way of establishing convergence is to derive some  bound on the distance or our sequence from the number that  we suspect to be the limit.  If that distance becomes smaller and smaller, if we can  manage to bound that distance by a certain number and that  number goes down to 0, then it is guaranteed that since this  distance goes down to 0, that the sequence, ai,  converges to a.  And there's a variation of this argument, which is the  so-called sandwich argument, and it goes as follows.  If we have a certain sequence that converges to some number,  a, and we have another sequence that converges to  that same number, a, and our sequence is somewhere  in-between, then our sequence must also converge to that  particular number, a.  So these are the usual ways of quickly saying something about  the convergence of a given sequence, and we will be often  using this type of argument in this class, but without making  a big fuss about them, or without even referring to  these facts in an explicit manner. 